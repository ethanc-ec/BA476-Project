{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from itertools import repeat\n",
    "from pathlib import Path\n",
    "\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the supplementary data from Kaggle for artist info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Lead Streams</th>\n",
       "      <th>Feats</th>\n",
       "      <th>Tracks</th>\n",
       "      <th>One Billion</th>\n",
       "      <th>100 Million</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drake</td>\n",
       "      <td>50162292808</td>\n",
       "      <td>19246513666</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bad Bunny</td>\n",
       "      <td>44369032140</td>\n",
       "      <td>5391990975</td>\n",
       "      <td>163</td>\n",
       "      <td>5</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>38153682361</td>\n",
       "      <td>2791278201</td>\n",
       "      <td>240</td>\n",
       "      <td>10</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>34767779741</td>\n",
       "      <td>4288903657</td>\n",
       "      <td>186</td>\n",
       "      <td>8</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>32596728109</td>\n",
       "      <td>424053296</td>\n",
       "      <td>323</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Artist Name  Lead Streams        Feats  Tracks  One Billion  100 Million\n",
       "0         Drake   50162292808  19246513666     262            6          130\n",
       "1     Bad Bunny   44369032140   5391990975     163            5          118\n",
       "2    Ed Sheeran   38153682361   2791278201     240           10           62\n",
       "3    The Weeknd   34767779741   4288903657     186            8           72\n",
       "4  Taylor Swift   32596728109    424053296     323            1           96"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if Path(\"data/spotify_artist_data.csv\").exists():\n",
    "    artist_stats = pd.read_csv(\"data/spotify_artist_data.csv\")\n",
    "else:\n",
    "    path = kagglehub.dataset_download(\"adnananam/spotify-artist-stats\")\n",
    "    artist_stats = pd.read_csv(path + \"/spotify_artist_data.csv\", index_col=0)\n",
    "\n",
    "    # Remove error rows b/c the creator didn't process correctly\n",
    "    artist_stats = artist_stats[artist_stats[\"Lead Streams\"] != \"Lead Streams\"]\n",
    "\n",
    "    # Cast numeric columns to int\n",
    "    for col in [\"Lead Streams\", \"Feats\", \"Tracks\", \"One Billion\", \"100 Million\"]:\n",
    "        artist_stats[col] = artist_stats[col].str.replace(\",\", \"\").astype(int)\n",
    "\n",
    "    # Remove the last updated column, it's not useful/relevant\n",
    "    artist_stats = artist_stats.drop(columns=[\"Last Updated\"])\n",
    "\n",
    "    artist_stats.to_csv(\"data/spotify_artist_data.csv\", index=False)\n",
    "\n",
    "artist_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mbid</th>\n",
       "      <th>artist_mb</th>\n",
       "      <th>artist_lastfm</th>\n",
       "      <th>country_mb</th>\n",
       "      <th>country_lastfm</th>\n",
       "      <th>tags_mb</th>\n",
       "      <th>tags_lastfm</th>\n",
       "      <th>listeners_lastfm</th>\n",
       "      <th>scrobbles_lastfm</th>\n",
       "      <th>ambiguous_artist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cc197bad-dc9c-440d-a5b5-d52ba2e14234</td>\n",
       "      <td>Coldplay</td>\n",
       "      <td>Coldplay</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>rock; pop; alternative rock; british; uk; brit...</td>\n",
       "      <td>rock; alternative; britpop; alternative rock; ...</td>\n",
       "      <td>5381567.0</td>\n",
       "      <td>360111850.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a74b1b7f-71a5-4011-9441-d0b5e4122711</td>\n",
       "      <td>Radiohead</td>\n",
       "      <td>Radiohead</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>rock; electronic; alternative rock; british; g...</td>\n",
       "      <td>alternative; alternative rock; rock; indie; el...</td>\n",
       "      <td>4732528.0</td>\n",
       "      <td>499548797.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8bfac288-ccc5-448d-9573-c33ea2aa5c30</td>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States</td>\n",
       "      <td>rock; alternative rock; 80s; 90s; rap; metal; ...</td>\n",
       "      <td>rock; alternative rock; alternative; Funk Rock...</td>\n",
       "      <td>4620835.0</td>\n",
       "      <td>293784041.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73e5e69d-3554-40d8-8516-00cb38737a1c</td>\n",
       "      <td>Rihanna</td>\n",
       "      <td>Rihanna</td>\n",
       "      <td>United States</td>\n",
       "      <td>Barbados; United States</td>\n",
       "      <td>pop; dance; hip hop; reggae; contemporary r b;...</td>\n",
       "      <td>pop; rnb; female vocalists; dance; Hip-Hop; Ri...</td>\n",
       "      <td>4558193.0</td>\n",
       "      <td>199248986.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b95ce3ff-3d05-4e87-9e01-c97b66af13d4</td>\n",
       "      <td>Eminem</td>\n",
       "      <td>Eminem</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States</td>\n",
       "      <td>turkish; rap; american; hip-hop; hip hop; hiph...</td>\n",
       "      <td>rap; Hip-Hop; Eminem; hip hop; pop; american; ...</td>\n",
       "      <td>4517997.0</td>\n",
       "      <td>199507511.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   mbid              artist_mb  \\\n",
       "0  cc197bad-dc9c-440d-a5b5-d52ba2e14234               Coldplay   \n",
       "1  a74b1b7f-71a5-4011-9441-d0b5e4122711              Radiohead   \n",
       "2  8bfac288-ccc5-448d-9573-c33ea2aa5c30  Red Hot Chili Peppers   \n",
       "3  73e5e69d-3554-40d8-8516-00cb38737a1c                Rihanna   \n",
       "4  b95ce3ff-3d05-4e87-9e01-c97b66af13d4                 Eminem   \n",
       "\n",
       "           artist_lastfm      country_mb           country_lastfm  \\\n",
       "0               Coldplay  United Kingdom           United Kingdom   \n",
       "1              Radiohead  United Kingdom           United Kingdom   \n",
       "2  Red Hot Chili Peppers   United States            United States   \n",
       "3                Rihanna   United States  Barbados; United States   \n",
       "4                 Eminem   United States            United States   \n",
       "\n",
       "                                             tags_mb  \\\n",
       "0  rock; pop; alternative rock; british; uk; brit...   \n",
       "1  rock; electronic; alternative rock; british; g...   \n",
       "2  rock; alternative rock; 80s; 90s; rap; metal; ...   \n",
       "3  pop; dance; hip hop; reggae; contemporary r b;...   \n",
       "4  turkish; rap; american; hip-hop; hip hop; hiph...   \n",
       "\n",
       "                                         tags_lastfm  listeners_lastfm  \\\n",
       "0  rock; alternative; britpop; alternative rock; ...         5381567.0   \n",
       "1  alternative; alternative rock; rock; indie; el...         4732528.0   \n",
       "2  rock; alternative rock; alternative; Funk Rock...         4620835.0   \n",
       "3  pop; rnb; female vocalists; dance; Hip-Hop; Ri...         4558193.0   \n",
       "4  rap; Hip-Hop; Eminem; hip hop; pop; american; ...         4517997.0   \n",
       "\n",
       "   scrobbles_lastfm  ambiguous_artist  \n",
       "0       360111850.0             False  \n",
       "1       499548797.0             False  \n",
       "2       293784041.0             False  \n",
       "3       199248986.0             False  \n",
       "4       199507511.0             False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if Path(\"data/artists.csv\").exists():\n",
    "    art_pop = pd.read_csv(\"data/artists.csv\", low_memory=False)\n",
    "else:\n",
    "    path = kagglehub.dataset_download(\"pieca111/music-artists-popularity\")\n",
    "    art_pop = pd.read_csv(path + \"/artists.csv\", low_memory=False)\n",
    "    art_pop.to_csv(\"data/artists.csv\", index=False)\n",
    "\n",
    "art_pop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from HuggingFace using Pandas, and drop the extra index column. The `na`/`NaN` values were dropped from the `artists` column because that column is used to merge the supplementary data above with the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "      <th>duration_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5SuOikwiRyPMVoIQDJUgSV</td>\n",
       "      <td>Gen Hoshino</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>73</td>\n",
       "      <td>False</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.715</td>\n",
       "      <td>87.917</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>230.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4qPNDBW1i3p13qLCt0Ki3A</td>\n",
       "      <td>Ben Woodward</td>\n",
       "      <td>Ghost (Acoustic)</td>\n",
       "      <td>Ghost - Acoustic</td>\n",
       "      <td>55</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>77.489</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>149.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1iJBSr7s7jYXzM8EGcbK5b</td>\n",
       "      <td>Ingrid Michaelson;ZAYN</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>57</td>\n",
       "      <td>False</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>76.332</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>210.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6lfxq3CG4xtTiEg7opyCyx</td>\n",
       "      <td>Kina Grannis</td>\n",
       "      <td>Crazy Rich Asians (Original Motion Picture Sou...</td>\n",
       "      <td>Can't Help Falling In Love</td>\n",
       "      <td>71</td>\n",
       "      <td>False</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>181.740</td>\n",
       "      <td>3</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>201.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5vjLSffimiIP26QG5WcN2K</td>\n",
       "      <td>Chord Overstreet</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>82</td>\n",
       "      <td>False</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>119.949</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>198.853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 track_id                 artists  \\\n",
       "0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n",
       "1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
       "2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
       "3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
       "4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
       "\n",
       "                                          album_name  \\\n",
       "0                                             Comedy   \n",
       "1                                   Ghost (Acoustic)   \n",
       "2                                     To Begin Again   \n",
       "3  Crazy Rich Asians (Original Motion Picture Sou...   \n",
       "4                                            Hold On   \n",
       "\n",
       "                   track_name  popularity  explicit  danceability  energy  \\\n",
       "0                      Comedy          73     False         0.676  0.4610   \n",
       "1            Ghost - Acoustic          55     False         0.420  0.1660   \n",
       "2              To Begin Again          57     False         0.438  0.3590   \n",
       "3  Can't Help Falling In Love          71     False         0.266  0.0596   \n",
       "4                     Hold On          82     False         0.618  0.4430   \n",
       "\n",
       "   key  loudness  mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
       "0    1    -6.746     0       0.1430        0.0322          0.000001    0.3580   \n",
       "1    1   -17.235     1       0.0763        0.9240          0.000006    0.1010   \n",
       "2    0    -9.734     1       0.0557        0.2100          0.000000    0.1170   \n",
       "3    0   -18.515     1       0.0363        0.9050          0.000071    0.1320   \n",
       "4    2    -9.681     1       0.0526        0.4690          0.000000    0.0829   \n",
       "\n",
       "   valence    tempo  time_signature track_genre  duration_s  \n",
       "0    0.715   87.917               4    acoustic     230.666  \n",
       "1    0.267   77.489               4    acoustic     149.610  \n",
       "2    0.120   76.332               4    acoustic     210.826  \n",
       "3    0.143  181.740               3    acoustic     201.933  \n",
       "4    0.167  119.949               4    acoustic     198.853  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pulled dataset from HF, dropped unneeded index column\n",
    "if Path(\"data/spotify_tracks.csv\").exists():\n",
    "    df = pd.read_csv(\"data/spotify_tracks.csv\")\n",
    "else:\n",
    "    df = (\n",
    "        pd.read_csv(\"hf://datasets/maharshipandya/spotify-tracks-dataset/dataset.csv\")\n",
    "        .drop(\"Unnamed: 0\", axis=1)\n",
    "        .dropna(subset=[\"artists\"])\n",
    "    )\n",
    "\n",
    "    df[\"duration_s\"] = df[\"duration_ms\"] / 1000\n",
    "    df = df.drop(columns=[\"duration_ms\"])  # Drop original duration column, keep seconds\n",
    "\n",
    "    df.to_csv(\"data/spotify_tracks.csv\", index=False)\n",
    "\n",
    "df_nodupe = df.drop_duplicates(subset=[\"track_id\"]).copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ecef9eb05b4c3c85e5ab2faa78b7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/89740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "art_pop_name = set(art_pop[\"artist_mb\"].values)\n",
    "listeners = []\n",
    "\n",
    "for row in tqdm(df_nodupe.iterrows(), total=df_nodupe.shape[0], desc=\"Processing rows\"):\n",
    "    artists = [x.strip() for x in row[1][\"artists\"].split(\";\")]\n",
    "    temp = []\n",
    "    for artist in artists:\n",
    "        if artist in art_pop_name:\n",
    "            temp.append(art_pop[art_pop[\"artist_mb\"] == artist][\"listeners_lastfm\"].values[0])\n",
    "    \n",
    "    if len(temp) == 0:\n",
    "        listeners.append(0)\n",
    "    else:\n",
    "        listeners.append(np.mean(temp))\n",
    "        \n",
    "df_nodupe[\"listeners\"] = listeners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodupe = df_nodupe.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "track_id            0\n",
       "artists             0\n",
       "album_name          0\n",
       "track_name          0\n",
       "popularity          0\n",
       "explicit            0\n",
       "danceability        0\n",
       "energy              0\n",
       "key                 0\n",
       "loudness            0\n",
       "mode                0\n",
       "speechiness         0\n",
       "acousticness        0\n",
       "instrumentalness    0\n",
       "liveness            0\n",
       "valence             0\n",
       "tempo               0\n",
       "time_signature      0\n",
       "track_genre         0\n",
       "duration_s          0\n",
       "listeners           0\n",
       "lead_streams        0\n",
       "feats               0\n",
       "tracks              0\n",
       "one_billion         0\n",
       "hundred_million     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nodupe.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in more information to the main dataset using each artist's stats. If there are two or more artists present, the stats are averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da980002637d49cb81b7521498a8568d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rows:   0%|          | 0/89740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'track_genre'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/BA476-Project/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'track_genre'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m df_nodupe[\u001b[33m\"\u001b[39m\u001b[33mone_billion\u001b[39m\u001b[33m\"\u001b[39m] = one_billion\n\u001b[32m     37\u001b[39m df_nodupe[\u001b[33m\"\u001b[39m\u001b[33mhundred_million\u001b[39m\u001b[33m\"\u001b[39m] = hundred_million\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m g_dummy = pd.get_dummies(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrack_genre\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m).groupby(df[\u001b[33m\"\u001b[39m\u001b[33mtrack_id\u001b[39m\u001b[33m\"\u001b[39m]).sum().astype(\u001b[38;5;28mint\u001b[39m).reset_index()\n\u001b[32m     41\u001b[39m dummy_val = g_dummy.copy()\n\u001b[32m     42\u001b[39m dummy_val[\u001b[33m\"\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m\"\u001b[39m] = dummy_val.sum(axis=\u001b[32m1\u001b[39m, numeric_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/BA476-Project/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/BA476-Project/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'track_genre'"
     ]
    }
   ],
   "source": [
    "if not Path(\"data/spotify_tracks_processed.csv\").exists():\n",
    "    art_stats_name = set(artist_stats[\"Artist Name\"].values)\n",
    "    lead_streams, feats, tracks, one_billion, hundred_million = [], [], [], [], []\n",
    "\n",
    "    for row in tqdm(df_nodupe.iterrows(), total=df_nodupe.shape[0], desc=\"Processing rows\"):\n",
    "        artists = [x.strip() for x in row[1][\"artists\"].split(\";\")]\n",
    "        temp_lead_streams, temp_feats, temp_tracks, temp_one_billion, temp_hundred_million = [], [], [], [], []\n",
    "\n",
    "        for artist in artists:\n",
    "            if artist in art_stats_name:\n",
    "                temp_lead_streams.append(\n",
    "                    artist_stats[artist_stats[\"Artist Name\"] == artist][\"Lead Streams\"].values[0]\n",
    "                )\n",
    "                temp_feats.append(artist_stats[artist_stats[\"Artist Name\"] == artist][\"Feats\"].values[0])\n",
    "                temp_tracks.append(artist_stats[artist_stats[\"Artist Name\"] == artist][\"Tracks\"].values[0])\n",
    "                temp_one_billion.append(\n",
    "                    artist_stats[artist_stats[\"Artist Name\"] == artist][\"One Billion\"].values[0]\n",
    "                )\n",
    "                temp_hundred_million.append(\n",
    "                    artist_stats[artist_stats[\"Artist Name\"] == artist][\"100 Million\"].values[0]\n",
    "                )\n",
    "\n",
    "        for col, temp in zip(\n",
    "            [lead_streams, feats, tracks, one_billion, hundred_million],\n",
    "            [temp_lead_streams, temp_feats, temp_tracks, temp_one_billion, temp_hundred_million],\n",
    "            strict=True,\n",
    "        ):\n",
    "            if len(temp) > 0:\n",
    "                col.append(np.mean(temp))\n",
    "            else:\n",
    "                col.append(0)\n",
    "            \n",
    "    df_nodupe[\"lead_streams\"] = lead_streams\n",
    "    df_nodupe[\"feats\"] = feats\n",
    "    df_nodupe[\"tracks\"] = tracks\n",
    "    df_nodupe[\"one_billion\"] = one_billion\n",
    "    df_nodupe[\"hundred_million\"] = hundred_million\n",
    "\n",
    "    g_dummy = pd.get_dummies(df[\"track_genre\"]).groupby(df[\"track_id\"]).sum().astype(int).reset_index()\n",
    "\n",
    "    dummy_val = g_dummy.copy()\n",
    "    dummy_val[\"total\"] = dummy_val.sum(axis=1, numeric_only=True)\n",
    "    dummy_val = dummy_val[[\"track_id\", \"total\"]].sort_values(\"track_id\", ascending=True)\n",
    "\n",
    "    process_check = (\n",
    "        df.groupby(\"track_id\")\n",
    "        .size()\n",
    "        .to_frame(\"total\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"track_id\", ascending=True)\n",
    "    )\n",
    "\n",
    "    for df1, df2 in zip(process_check.iterrows(), dummy_val.iterrows(), strict=True):\n",
    "        assert (df1[1][\"total\"] == df2[1][\"total\"]) and (df1[1][\"track_id\"] == df2[1][\"track_id\"])\n",
    "\n",
    "    df = df_nodupe.merge(g_dummy, on=\"track_id\").drop(\n",
    "        [\"track_id\", \"artists\", \"album_name\", \"track_name\", \"track_genre\"], axis=1\n",
    "    )\n",
    "    df[\"explicit\"] = df[\"explicit\"].astype(int)\n",
    "    df.to_csv(\"data/spotify_tracks_processed.csv\", index=False)\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv(\"data/spotify_tracks_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>...</th>\n",
       "      <th>spanish</th>\n",
       "      <th>study</th>\n",
       "      <th>swedish</th>\n",
       "      <th>synth-pop</th>\n",
       "      <th>tango</th>\n",
       "      <th>techno</th>\n",
       "      <th>trance</th>\n",
       "      <th>trip-hop</th>\n",
       "      <th>turkish</th>\n",
       "      <th>world-music</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "      <td>89740.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>33.198808</td>\n",
       "      <td>0.085848</td>\n",
       "      <td>0.562166</td>\n",
       "      <td>0.634458</td>\n",
       "      <td>5.283530</td>\n",
       "      <td>-8.498994</td>\n",
       "      <td>0.636973</td>\n",
       "      <td>0.087442</td>\n",
       "      <td>0.328285</td>\n",
       "      <td>0.173415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011143</td>\n",
       "      <td>0.011143</td>\n",
       "      <td>0.011143</td>\n",
       "      <td>0.011143</td>\n",
       "      <td>0.011143</td>\n",
       "      <td>0.011143</td>\n",
       "      <td>0.011143</td>\n",
       "      <td>0.011143</td>\n",
       "      <td>0.011143</td>\n",
       "      <td>0.011143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>20.580640</td>\n",
       "      <td>0.280141</td>\n",
       "      <td>0.176692</td>\n",
       "      <td>0.256606</td>\n",
       "      <td>3.559912</td>\n",
       "      <td>5.221518</td>\n",
       "      <td>0.480875</td>\n",
       "      <td>0.113278</td>\n",
       "      <td>0.338321</td>\n",
       "      <td>0.323849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104973</td>\n",
       "      <td>0.105185</td>\n",
       "      <td>0.104973</td>\n",
       "      <td>0.104973</td>\n",
       "      <td>0.104973</td>\n",
       "      <td>0.104973</td>\n",
       "      <td>0.105079</td>\n",
       "      <td>0.105291</td>\n",
       "      <td>0.105079</td>\n",
       "      <td>0.105079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-49.531000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-10.322250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576000</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-7.185000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-5.108000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.085900</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.097625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.532000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         popularity      explicit  danceability        energy           key  \\\n",
       "count  89740.000000  89740.000000  89740.000000  89740.000000  89740.000000   \n",
       "mean      33.198808      0.085848      0.562166      0.634458      5.283530   \n",
       "std       20.580640      0.280141      0.176692      0.256606      3.559912   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       19.000000      0.000000      0.450000      0.457000      2.000000   \n",
       "50%       33.000000      0.000000      0.576000      0.676000      5.000000   \n",
       "75%       49.000000      0.000000      0.692000      0.853000      8.000000   \n",
       "max      100.000000      1.000000      0.985000      1.000000     11.000000   \n",
       "\n",
       "           loudness          mode   speechiness  acousticness  \\\n",
       "count  89740.000000  89740.000000  89740.000000  89740.000000   \n",
       "mean      -8.498994      0.636973      0.087442      0.328285   \n",
       "std        5.221518      0.480875      0.113278      0.338321   \n",
       "min      -49.531000      0.000000      0.000000      0.000000   \n",
       "25%      -10.322250      0.000000      0.036000      0.017100   \n",
       "50%       -7.185000      1.000000      0.048900      0.188000   \n",
       "75%       -5.108000      1.000000      0.085900      0.625000   \n",
       "max        4.532000      1.000000      0.965000      0.996000   \n",
       "\n",
       "       instrumentalness  ...       spanish         study       swedish  \\\n",
       "count      89740.000000  ...  89740.000000  89740.000000  89740.000000   \n",
       "mean           0.173415  ...      0.011143      0.011143      0.011143   \n",
       "std            0.323849  ...      0.104973      0.105185      0.104973   \n",
       "min            0.000000  ...      0.000000      0.000000      0.000000   \n",
       "25%            0.000000  ...      0.000000      0.000000      0.000000   \n",
       "50%            0.000058  ...      0.000000      0.000000      0.000000   \n",
       "75%            0.097625  ...      0.000000      0.000000      0.000000   \n",
       "max            1.000000  ...      1.000000      2.000000      1.000000   \n",
       "\n",
       "          synth-pop         tango        techno        trance      trip-hop  \\\n",
       "count  89740.000000  89740.000000  89740.000000  89740.000000  89740.000000   \n",
       "mean       0.011143      0.011143      0.011143      0.011143      0.011143   \n",
       "std        0.104973      0.104973      0.104973      0.105079      0.105291   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      2.000000      2.000000   \n",
       "\n",
       "            turkish   world-music  \n",
       "count  89740.000000  89740.000000  \n",
       "mean       0.011143      0.011143  \n",
       "std        0.105079      0.105079  \n",
       "min        0.000000      0.000000  \n",
       "25%        0.000000      0.000000  \n",
       "50%        0.000000      0.000000  \n",
       "75%        0.000000      0.000000  \n",
       "max        2.000000      2.000000  \n",
       "\n",
       "[8 rows x 135 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "popularity          1.000000\n",
       "pop-film            0.134407\n",
       "k-pop               0.122339\n",
       "hundred_million     0.106079\n",
       "chill               0.105386\n",
       "                      ...   \n",
       "detroit-techno     -0.113376\n",
       "latin              -0.127165\n",
       "instrumentalness   -0.127477\n",
       "romance            -0.141027\n",
       "iranian            -0.157936\n",
       "Name: popularity, Length: 135, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()[\"popularity\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeros in lead_streams: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 200 out of 200 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Create mask for rows where lead_streams is 0\n",
    "mask = df['lead_streams'] == 0\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X_train = df[~mask].drop(['lead_streams', 'popularity'], axis=1)\n",
    "y_train = df[~mask]['lead_streams']\n",
    "\n",
    "# Prepare features for prediction\n",
    "X_pred = df[mask].drop(['lead_streams', 'popularity'], axis=1)\n",
    "\n",
    "# Initialize and train the RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200, \n",
    "    random_state=42, \n",
    "    n_jobs=-1, \n",
    "    max_features='sqrt',\n",
    "    verbose=1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for empty values\n",
    "predictions = rf_model.predict(X_pred)\n",
    "\n",
    "# Fill in the empty values\n",
    "df.loc[mask, 'lead_streams'] = predictions\n",
    "\n",
    "# Verify no more zeros in lead_streams\n",
    "print(f\"Number of zeros in lead_streams: {(df['lead_streams'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m std_df = StandardScaler().fit_transform(df[df.columns.difference([\u001b[33m\"\u001b[39m\u001b[33mpopularity\u001b[39m\u001b[33m\"\u001b[39m])])\n\u001b[32m      2\u001b[39m kmeans = KMeans(n_clusters=\u001b[32m40\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mkmeans\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m] = kmeans.labels_\n\u001b[32m      5\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/BA476-Project/.venv/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/BA476-Project/.venv/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:1454\u001b[39m, in \u001b[36mKMeans.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1428\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[32m   1429\u001b[39m \n\u001b[32m   1430\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1452\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1453\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1454\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1457\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1464\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_params_vs_input(X)\n\u001b[32m   1466\u001b[39m     random_state = check_random_state(\u001b[38;5;28mself\u001b[39m.random_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/BA476-Project/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/BA476-Project/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/BA476-Project/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GitHub/BA476-Project/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "std_df = StandardScaler().fit_transform(df[df.columns.difference([\"popularity\"])])\n",
    "kmeans = KMeans(n_clusters=40, random_state=42)\n",
    "kmeans.fit(std_df)\n",
    "df[\"cluster\"] = kmeans.labels_\n",
    "df[\"cluster\"] = df[\"cluster\"].astype(\"category\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = df.corr()\n",
    "\n",
    "# Create mask for correlations > abs(0.50)\n",
    "mask = np.abs(dfc) > 0.50\n",
    "\n",
    "# Get upper triangle of mask to avoid duplicates\n",
    "mask_upper = np.triu(mask, k=1)\n",
    "\n",
    "# Find correlation pairs exceeding threshold\n",
    "high_corr = []\n",
    "for i in range(len(dfc.columns)):\n",
    "    for j in range(i + 1, len(dfc.columns)):\n",
    "        if mask_upper[i, j]:\n",
    "            high_corr.append({\"var1\": dfc.columns[i], \"var2\": dfc.columns[j], \"corr\": dfc.iloc[i, j]})\n",
    "\n",
    "# Convert to dataframe and sort by absolute correlation\n",
    "high_corr_df = pd.DataFrame(high_corr)\n",
    "high_corr_df = high_corr_df.sort_values(\"corr\", key=abs, ascending=False)\n",
    "\n",
    "print(\"Correlations > |0.50|:\")\n",
    "print(high_corr_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"singer-songwriter\", \"hundred_million\", \"one_billion\", \"feats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'features' is a list of feature column names\n",
    "X = df[df.columns.difference([\"popularity\"])]\n",
    "y = df[\"popularity\"]\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=200, random_state=42, n_jobs=-1, verbose=1, max_features=\"sqrt\", bootstrap=True\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R² and MSE\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f\"R²: {r2}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 50\n",
    "\n",
    "# Get feature importances and column names\n",
    "feature_importances = pd.DataFrame(\n",
    "    {\"feature\": X_train.columns, \"importance\": model.feature_importances_}\n",
    ")\n",
    "\n",
    "# Sort by importance and get top_n\n",
    "top_features = feature_importances.sort_values(\"importance\", ascending=False).head(top_n)\n",
    "\n",
    "# Display results\n",
    "print(f\"Top {top_n} most important features:\")\n",
    "print(top_features.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'features' is a list of feature column names\n",
    "X = df[top_features[\"feature\"].to_list()]\n",
    "y = df[\"popularity\"]\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=200, random_state=42, n_jobs=-1, verbose=1, max_features=\"sqrt\", bootstrap=True\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R² and MSE\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f\"R²: {r2}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can skip stacked model maybe, its performance is only slightly above the RFRs.\n",
    "\n",
    "```md\n",
    "Stacked Model R^2: 0.537192088407299\n",
    "Stacked Model MSE: 194.77277721078033\n",
    "Stacked Model RMSE: 13.956101791359231\n",
    "Stacked Model MAE: 9.468840211962455\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splitting the data\n",
    "# X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(\n",
    "#     df[top_features[\"feature\"].to_list()], df[\"popularity\"], test_size=0.3, random_state=42\n",
    "# )\n",
    "\n",
    "# # Base models\n",
    "# base_models = [\n",
    "#     (\"rf\", RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)),\n",
    "#     (\"gb\", GradientBoostingRegressor(n_estimators=200, random_state=42)),\n",
    "#     (\"lr\", LinearRegression()),\n",
    "# ]\n",
    "\n",
    "# # Meta-model (Level 2 model)\n",
    "# meta_model = LinearRegression()\n",
    "\n",
    "# # Stacking model\n",
    "# stacked_model = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
    "# stacked_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Predictions and MSE\n",
    "# y_pred_stacked = stacked_model.predict(X_test_scaled)\n",
    "# mse_stacked = mean_squared_error(y_test, y_pred_stacked)\n",
    "# print(f\"Stacked Model MSE: {mse_stacked}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2 = r2_score(y_test, y_pred_stacked)\n",
    "# mse = mean_squared_error(y_test, y_pred_stacked)\n",
    "# rmse = root_mean_squared_error(y_test, y_pred_stacked)\n",
    "# mae = mean_absolute_error(y_test, y_pred_stacked)\n",
    "\n",
    "# print(f\"Stacked Model R^2: {r2}\")\n",
    "# print(f\"Stacked Model MSE: {mse}\")\n",
    "# print(f\"Stacked Model RMSE: {rmse}\")\n",
    "# print(f\"Stacked Model MAE: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
