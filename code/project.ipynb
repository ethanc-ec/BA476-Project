{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BA 476 Team 10 Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".cell-output-ipywidget-background {\n",
    "    background-color: transparent !important;\n",
    "}\n",
    ":root {\n",
    "    --jp-widgets-color: var(--vscode-editor-foreground);\n",
    "    --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
    "}  \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "from xgboost import XGBRegressor, XGBRFRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To collect results from all models as we go\n",
    "full_results = pd.DataFrame()\n",
    "\n",
    "# Color palette\n",
    "sns.set(\n",
    "    rc={\n",
    "        \"axes.facecolor\": (1, 1, 1, 0),\n",
    "        \"figure.facecolor\": (1, 1, 1, 0),\n",
    "        \"text.color\": \"lightgray\",\n",
    "        \"xtick.color\": \"lightgray\",\n",
    "        \"ytick.color\": \"lightgray\",\n",
    "    }\n",
    ")\n",
    "spotify_palette = sns.diverging_palette(279, 141, s=92, l=68, center=\"light\", as_cmap=True)\n",
    "spotify_colors = sns.diverging_palette(279, 141, s=92, l=68, center=\"light\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download and Processing\n",
    "Download the supplementary data from Kaggle for artist info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(\"data/spotify_artist_data.csv\").exists():\n",
    "    artist_stats = pd.read_csv(\"data/spotify_artist_data.csv\")\n",
    "else:\n",
    "    path = kagglehub.dataset_download(\"adnananam/spotify-artist-stats\")\n",
    "    artist_stats = pd.read_csv(path + \"/spotify_artist_data.csv\", index_col=0)\n",
    "\n",
    "    # Remove error rows b/c the creator didn't process correctly\n",
    "    artist_stats = artist_stats[artist_stats[\"Lead Streams\"] != \"Lead Streams\"]\n",
    "\n",
    "    # Cast numeric columns to int\n",
    "    for col in [\"Lead Streams\", \"Feats\", \"Tracks\", \"One Billion\", \"100 Million\"]:\n",
    "        artist_stats[col] = artist_stats[col].str.replace(\",\", \"\").astype(int)\n",
    "\n",
    "    # Remove the last updated column, it's not useful/relevant\n",
    "    artist_stats = artist_stats.drop(columns=[\"Last Updated\"])\n",
    "\n",
    "    artist_stats.to_csv(\"data/spotify_artist_data.csv\", index=False)\n",
    "\n",
    "artist_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from HuggingFace using Pandas, and drop the extra index column. The `na`/`NaN` values were dropped from the `artists` column because that column is used to merge the supplementary data above with the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulled dataset from HF, dropped unneeded index column\n",
    "if Path(\"data/spotify_tracks.csv\").exists():\n",
    "    df = pd.read_csv(\"data/spotify_tracks.csv\")\n",
    "else:\n",
    "    df = (\n",
    "        pd.read_csv(\"hf://datasets/maharshipandya/spotify-tracks-dataset/dataset.csv\")\n",
    "        .drop(\"Unnamed: 0\", axis=1)\n",
    "        .dropna(subset=[\"artists\"])\n",
    "    )\n",
    "\n",
    "    df[\"duration_s\"] = df[\"duration_ms\"] / 1000\n",
    "    df = df.drop(columns=[\"duration_ms\"])  # Drop original duration column, keep seconds\n",
    "\n",
    "    df.to_csv(\"data/spotify_tracks.csv\", index=False)\n",
    "\n",
    "df_nodupe = df.drop_duplicates(subset=[\"track_id\"]).copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in more information to the main dataset using each artist's stats. If there are two or more artists present, the stats are averaged.\n",
    "\n",
    "Stats merged:\n",
    "\n",
    "- Lead streams\n",
    "- Streams of features\n",
    "- Number of tracks\n",
    "- Number of songs with more than one billion streams\n",
    "- Number of songs with more than 100 million streams\n",
    "\n",
    "The second half of the cell creates dummy variables for the `genres` column. The genre column and duplicate song entries are then dropped from the dataframe. Each song is repeated $x$ number of times where $x$ is the number of genres it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(\"data/spotify_tracks_processed.csv\").exists():\n",
    "    # Adding in information based on the artist stats (merge on names)\n",
    "    art_stats_name = set(artist_stats[\"Artist Name\"].values)\n",
    "    lead_streams, feats, tracks, one_billion, hundred_million = [], [], [], [], []\n",
    "\n",
    "    for row in tqdm(df_nodupe.iterrows(), total=df_nodupe.shape[0], desc=\"Processing rows\"):\n",
    "        artists = [x.strip() for x in row[1][\"artists\"].split(\";\")]\n",
    "        temp_lead_streams, temp_feats, temp_tracks, temp_one_billion, temp_hundred_million = (\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "            [],\n",
    "        )\n",
    "\n",
    "        for artist in artists:\n",
    "            if artist in art_stats_name:\n",
    "                temp_lead_streams.append(\n",
    "                    artist_stats[artist_stats[\"Artist Name\"] == artist][\"Lead Streams\"].values[0]\n",
    "                )\n",
    "                temp_feats.append(artist_stats[artist_stats[\"Artist Name\"] == artist][\"Feats\"].values[0])\n",
    "                temp_tracks.append(\n",
    "                    artist_stats[artist_stats[\"Artist Name\"] == artist][\"Tracks\"].values[0]\n",
    "                )\n",
    "                temp_one_billion.append(\n",
    "                    artist_stats[artist_stats[\"Artist Name\"] == artist][\"One Billion\"].values[0]\n",
    "                )\n",
    "                temp_hundred_million.append(\n",
    "                    artist_stats[artist_stats[\"Artist Name\"] == artist][\"100 Million\"].values[0]\n",
    "                )\n",
    "\n",
    "        for col, temp in zip(\n",
    "            [lead_streams, feats, tracks, one_billion, hundred_million],\n",
    "            [temp_lead_streams, temp_feats, temp_tracks, temp_one_billion, temp_hundred_million],\n",
    "            strict=True,\n",
    "        ):\n",
    "            if len(temp) > 0:\n",
    "                col.append(np.mean(temp))\n",
    "            else:\n",
    "                col.append(0)\n",
    "\n",
    "    df_nodupe[\"lead_streams\"] = lead_streams\n",
    "    df_nodupe[\"feats\"] = feats\n",
    "    df_nodupe[\"tracks\"] = tracks\n",
    "    df_nodupe[\"one_billion\"] = one_billion\n",
    "    df_nodupe[\"hundred_million\"] = hundred_million\n",
    "\n",
    "\n",
    "    # Creating dummy variables based on genres\n",
    "    g_dummy = pd.get_dummies(df[\"track_genre\"]).groupby(df[\"track_id\"]).sum().astype(int).reset_index()\n",
    "\n",
    "    dummy_val = g_dummy.copy()\n",
    "    dummy_val[\"total\"] = dummy_val.sum(axis=1, numeric_only=True)\n",
    "    dummy_val = dummy_val[[\"track_id\", \"total\"]].sort_values(\"track_id\", ascending=True)\n",
    "\n",
    "    process_check = (\n",
    "        df.groupby(\"track_id\")\n",
    "        .size()\n",
    "        .to_frame(\"total\")\n",
    "        .reset_index()\n",
    "        .sort_values(\"track_id\", ascending=True)\n",
    "    )\n",
    "\n",
    "    for df1, df2 in zip(process_check.iterrows(), dummy_val.iterrows(), strict=True):\n",
    "        assert (df1[1][\"total\"] == df2[1][\"total\"]) and (df1[1][\"track_id\"] == df2[1][\"track_id\"])\n",
    "\n",
    "    df = df_nodupe.merge(g_dummy, on=\"track_id\").drop(\n",
    "        [\"track_id\", \"artists\", \"album_name\", \"track_name\", \"track_genre\"], axis=1\n",
    "    )\n",
    "    df[\"explicit\"] = df[\"explicit\"].astype(int)\n",
    "    df.to_csv(\"data/spotify_tracks_processed.csv\", index=False)\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv(\"data/spotify_tracks_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Linear Regression Model\n",
    "\n",
    "A quick test of the linear regression model using only the base data and dummy variables made from genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'features' is a list of feature column names\n",
    "X = df[\n",
    "    df.columns.difference(\n",
    "        [\"popularity\", \"lead_streams\", \"feats\", \"tracks\", \"one_billion\", \"hundred_million\"]\n",
    "    )\n",
    "]\n",
    "y = df[\"popularity\"]\n",
    "\n",
    "# Split the dataset into train and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Initialize the LinearRegression model\n",
    "model = LinearRegression(\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate R² and MSE\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "full_results = pd.concat(\n",
    "    [\n",
    "        full_results,\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Model\": \"Baseline Linear Regression\",\n",
    "                \"Dataset\": [\"Test\", \"Train\"],\n",
    "                \"R²\": [r2, r2_score(y_train, model.predict(X_train))],\n",
    "                \"MSE\": [mse, mean_squared_error(y_train, model.predict(X_train))],\n",
    "                \"RMSE\": [rmse, root_mean_squared_error(y_train, model.predict(X_train))],\n",
    "                \"MAE\": [mae, mean_absolute_error(y_train, model.predict(X_train))],\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Output the results\n",
    "print(f\"R²: {r2}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backfill missing `lead_streams` values\n",
    "\n",
    "Fill in values for `lead_streams` using all columns except for `lead_streams` and `popularity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Create mask for rows where lead_streams is 0\n",
    "mask = df['lead_streams'] == 0\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X_train = df[~mask].drop(['lead_streams', 'popularity'], axis=1)\n",
    "y_train = df[~mask]['lead_streams']\n",
    "\n",
    "# Prepare features for prediction\n",
    "X_pred = df[mask].drop(['lead_streams', 'popularity'], axis=1)\n",
    "\n",
    "# Initialize and train the RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200, \n",
    "    random_state=42, \n",
    "    n_jobs=-1, \n",
    "    max_features='sqrt',\n",
    "    verbose=1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for empty values\n",
    "predictions = rf_model.predict(X_pred)\n",
    "\n",
    "# Fill in the empty values\n",
    "df.loc[mask, 'lead_streams'] = predictions\n",
    "\n",
    "# Verify no more zeros in lead_streams\n",
    "print(f\"Number of zeros in lead_streams: {(df['lead_streams'] == 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add clusters as additional features\n",
    "\n",
    "KMeans clusters as an additonal feature for the models to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "std_df = StandardScaler().fit_transform(df[df.columns.difference([\"popularity\"])])\n",
    "kmeans = KMeans(n_clusters=40, random_state=42)\n",
    "kmeans.fit(std_df)\n",
    "df[\"cluster\"] = kmeans.labels_\n",
    "df[\"cluster\"] = df[\"cluster\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual correlation check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = df.corr()\n",
    "\n",
    "# Create mask for correlations > abs(0.50)\n",
    "mask = np.abs(dfc) > 0.50\n",
    "\n",
    "# Get upper triangle of mask to avoid duplicates\n",
    "mask_upper = np.triu(mask, k=1)\n",
    "\n",
    "# Find correlation pairs exceeding threshold\n",
    "high_corr = []\n",
    "for i in range(len(dfc.columns)):\n",
    "    for j in range(i + 1, len(dfc.columns)):\n",
    "        if mask_upper[i, j]:\n",
    "            high_corr.append({\"var1\": dfc.columns[i], \"var2\": dfc.columns[j], \"corr\": dfc.iloc[i, j]})\n",
    "\n",
    "# Convert to dataframe and sort by absolute correlation\n",
    "high_corr_df = pd.DataFrame(high_corr)\n",
    "high_corr_df = high_corr_df.sort_values(\"corr\", key=abs, ascending=False)\n",
    "\n",
    "print(\"Correlations > |0.50|:\")\n",
    "print(high_corr_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop highly correlated columns\n",
    "\n",
    "- `singer-songwriter`\n",
    "  - Removed since it is an identical match to `songwriter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"singer-songwriter\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Linear Regression\n",
    "\n",
    "This is another run of the Linear Regression Model but with using the data with extra features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'features' is a list of feature column names\n",
    "X = df[df.columns.difference([\"popularity\"])]\n",
    "y = df[\"popularity\"]\n",
    "\n",
    "# Split the dataset into train and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "model = LinearRegression(\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate R² and MSE\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "full_results = pd.concat(\n",
    "    [\n",
    "        full_results,\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Model\": \"Post-processing Linear Regression\",\n",
    "                \"Dataset\": [\"Test\", \"Train\"],\n",
    "                \"R²\": [r2, r2_score(y_train, model.predict(X_train))],\n",
    "                \"MSE\": [mse, mean_squared_error(y_train, model.predict(X_train))],\n",
    "                \"RMSE\": [rmse, root_mean_squared_error(y_train, model.predict(X_train))],\n",
    "                \"MAE\": [mae, mean_absolute_error(y_train, model.predict(X_train))],\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Output the results\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'features' is a list of feature column names\n",
    "X = df[df.columns.difference([\"popularity\"])]\n",
    "y = df[\"popularity\"]\n",
    "\n",
    "# Split the dataset into train and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor model\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=200, random_state=42, n_jobs=-1, max_features=\"sqrt\", bootstrap=True\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate R² and MSE\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "full_results = pd.concat(\n",
    "    [\n",
    "        full_results,\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Model\": \"Random Forest Regressor\",\n",
    "                \"Dataset\": [\"Test\", \"Train\"],\n",
    "                \"R²\": [r2, r2_score(y_train, model.predict(X_train))],\n",
    "                \"MSE\": [mse, mean_squared_error(y_train, model.predict(X_train))],\n",
    "                \"RMSE\": [rmse, root_mean_squared_error(y_train, model.predict(X_train))],\n",
    "                \"MAE\": [mae, mean_absolute_error(y_train, model.predict(X_train))],\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Output the results\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor\n",
    "\n",
    "Given this is a boosting model, `early_stopping_rounds` has been set to 50 to avoid overfitting on the train and validation data. But unlike the other models, this one gets a custom `75:15:10` train/validation/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'features' is a list of feature column names\n",
    "X = df[df.columns.difference([\"popularity\"])]\n",
    "y = df[\"popularity\"]\n",
    "\n",
    "# Split the dataset into train, test, and validation sets (75% train, 15% test, 10% validation)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=40, random_state=42)\n",
    "\n",
    "# Initialize the XGBRegressor model\n",
    "model = XGBRegressor(\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=300,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    enable_categorical=True,\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train, eval_set=[(X_val, y_val)])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R² and MSE\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "full_results = pd.concat(\n",
    "    [\n",
    "        full_results,\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Model\": \"XGBoost Regressor\",\n",
    "                \"Dataset\": [\"Test\", \"Train\"],\n",
    "                \"R²\": [r2, r2_score(y_train, model.predict(X_train))],\n",
    "                \"MSE\": [mse, mean_squared_error(y_train, model.predict(X_train))],\n",
    "                \"RMSE\": [rmse, root_mean_squared_error(y_train, model.predict(X_train))],\n",
    "                \"MAE\": [mae, mean_absolute_error(y_train, model.predict(X_train))],\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Output the results\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and 'features' is a list of feature column names\n",
    "X = df[df.columns.difference([\"popularity\"])]\n",
    "y = df[\"popularity\"]\n",
    "\n",
    "# Split the dataset into train and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Initialize the XGBRFRegressor model\n",
    "model = XGBRFRegressor(\n",
    "    tree_method=\"hist\", n_estimators=200, n_jobs=-1, random_state=42, enable_categorical=True\n",
    ")\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate R² and MSE\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "full_results = pd.concat(\n",
    "    [\n",
    "        full_results,\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Model\": \"XGBoost Random Forest Regressor\",\n",
    "                \"Dataset\": [\"Test\", \"Train\"],\n",
    "                \"R²\": [r2, r2_score(y_train, model.predict(X_train))],\n",
    "                \"MSE\": [mse, mean_squared_error(y_train, model.predict(X_train))],\n",
    "                \"RMSE\": [rmse, root_mean_squared_error(y_train, model.predict(X_train))],\n",
    "                \"MAE\": [mae, mean_absolute_error(y_train, model.predict(X_train))],\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Output the results\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of RMSE and MAE for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt_df = full_results[full_results[\"Dataset\"] == \"Test\"].sort_values(\"MSE\", ascending=False)\n",
    "\n",
    "# Create bar positions\n",
    "x = range(len(plt_df))\n",
    "\n",
    "# Create grouped bars for each metric with adjusted positions\n",
    "bar_width = 0.2\n",
    "bars1 = plt.bar(\n",
    "    [i - bar_width / 2 for i in x],\n",
    "    plt_df[\"RMSE\"],\n",
    "    bar_width,\n",
    "    label=\"RMSE\",\n",
    "    color=spotify_colors[-1],\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "bars2 = plt.bar(\n",
    "    [i + bar_width / 2 for i in x],\n",
    "    plt_df[\"MAE\"],\n",
    "    bar_width,\n",
    "    label=\"MAE\",\n",
    "    color=spotify_colors[0],\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.3,\n",
    "            f\"{height:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel(\"Models\", color=\"lightgray\", fontsize=12)\n",
    "plt.ylabel(\"Score\", color=\"lightgray\", fontsize=12)\n",
    "plt.title(\"Model Performance Comparison - RMSE and MAE\", fontsize=14)\n",
    "\n",
    "# Position x-ticks at the center of each model's bar group\n",
    "# Format labels with newlines between words\n",
    "labels = plt_df[\"Model\"].str.replace(\" \", \"\\n\", regex=False)\n",
    "plt.xticks(x, labels, rotation=0, ha=\"center\", fontsize=10)\n",
    "\n",
    "# Improve legend readability\n",
    "plt.legend(loc=\"upper right\", fontsize=12, frameon=True, facecolor=\"black\", edgecolor=\"gray\")\n",
    "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.grid(False, axis=\"x\")  # Disable grid for x-axis\n",
    "plt.ylim(0, 20)  # Set y-axis limit\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"images/rmse-mae.png\", dpi=300, bbox_inches=\"tight\", transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of MSE for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt_df = full_results[full_results[\"Dataset\"] == \"Test\"].sort_values(\"MSE\", ascending=False)\n",
    "\n",
    "# Create bar positions\n",
    "x = range(len(plt_df))\n",
    "\n",
    "# Create bar plot for MSE\n",
    "bars = plt.bar(x, plt_df[\"MSE\"], width=0.5, color=spotify_colors[-1], label=\"MSE\")\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2.0, height, f\"{height:.2f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel(\"Models\", color=\"lightgray\", fontsize=12)\n",
    "plt.ylabel(\"Score\", color=\"lightgray\", fontsize=12)\n",
    "plt.title(\"Model Performance Comparison - MSE\")\n",
    "\n",
    "# Position x-ticks at the center of each model's bar group\n",
    "# Format labels with newlines between words\n",
    "labels = plt_df[\"Model\"].str.replace(\" \", \"\\n\", regex=False)\n",
    "plt.xticks(x, labels, rotation=0, ha=\"center\", fontsize=10)\n",
    "\n",
    "plt.legend(loc=\"upper right\", fontsize=12, frameon=True, facecolor=\"black\", edgecolor=\"gray\")\n",
    "plt.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.grid(False, axis=\"x\")  # Disable grid for x-axis\n",
    "\n",
    "plt.ylim(0, 350)  # Set y-axis limit to 10% above max value\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"images/mse.png\", dpi=300, bbox_inches=\"tight\", transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique models from the results\n",
    "models = full_results[full_results['Dataset'] == 'Test']['Model'].unique()\n",
    "\n",
    "# Prepare X and y data\n",
    "X = df[df.columns.difference([\"popularity\"])]\n",
    "y = df[\"popularity\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Dictionary to store model instances\n",
    "model_instances = {\n",
    "    \"Baseline Linear Regression\": LinearRegression(n_jobs=-1),\n",
    "    \"Post-processing Linear Regression\": LinearRegression(n_jobs=-1),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(\n",
    "        n_estimators=200, random_state=42, n_jobs=-1, max_features=\"sqrt\"),\n",
    "    \"XGBoost Regressor\": XGBRegressor(\n",
    "        tree_method=\"hist\", n_estimators=300, n_jobs=-1, random_state=42, \n",
    "        enable_categorical=True),\n",
    "    \"XGBoost Random Forest Regressor\": XGBRFRegressor(\n",
    "        tree_method=\"hist\", n_estimators=200, n_jobs=-1, random_state=42, \n",
    "        enable_categorical=True)\n",
    "}\n",
    "\n",
    "# Create separate figures for each model\n",
    "for i, model_name in enumerate(models):\n",
    "    # Create a new figure for each model\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    \n",
    "    # Train the model\n",
    "    model = model_instances[model_name]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Create scatter plot of actual vs predicted values\n",
    "    plt.scatter(y_test, y_pred, alpha=0.3, color=spotify_colors[i % len(spotify_colors)])\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = 0\n",
    "    max_val = 100\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], '--', lw=2, color='cyan')\n",
    "    \n",
    "    # Calculate metrics for the title\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Set title and labels\n",
    "    plt.title(f'{model_name}\\nR² = {r2:.3f}, RMSE = {rmse:.3f}', fontsize=12, color='lightgray')\n",
    "    plt.xlabel('Actual Popularity', color='lightgray')\n",
    "    plt.ylabel('Predicted Popularity', color='lightgray')\n",
    "    \n",
    "    # Set tick colors\n",
    "    plt.tick_params(colors='lightgray')\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Set equal limits for better comparison\n",
    "    plt.xlim(0, 100)\n",
    "    plt.ylim(0, 100)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f\"images/predicted_vs_actual_{model_name.replace(' ', '_').lower()}.png\", \n",
    "                dpi=300, bbox_inches=\"tight\", transparent=True)\n",
    "\n",
    "# Show a message instead of displaying all plots at once\n",
    "print(f\"Created and saved {len(models)} individual model plots to images/ directory\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KFold Cross Validation\n",
    "\n",
    "Because the Random Forest Regressor performed the best, we will use it for KFold Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is already loaded and preprocessed\n",
    "target = \"popularity\"\n",
    "features = df.columns.difference([\"popularity\"])\n",
    "\n",
    "\n",
    "# Preprocessing function (No scaling needed for Random Forest)\n",
    "def preprocess_data(df, features, target):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Nested Cross-Validation function with GridSearchCV\n",
    "def nested_cv(model, param_grid, X, y, k_outer=5, k_inner=3):\n",
    "    outer_kf = KFold(n_splits=k_outer, shuffle=True, random_state=42)\n",
    "    outer_mses = []\n",
    "\n",
    "    # Store all parameter combinations and their corresponding MSE for visualization\n",
    "    param_combinations = []\n",
    "    mse_values = []\n",
    "\n",
    "    # Outer loop for cross-validation with tqdm progress bar\n",
    "    for train_index, test_index in tqdm(outer_kf.split(X), total=k_outer, desc=\"Outer loop\"):\n",
    "        X_train_outer, X_test_outer = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_outer, y_test_outer = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Inner loop for hyperparameter tuning using GridSearchCV\n",
    "        inner_kf = KFold(n_splits=k_inner, shuffle=True, random_state=42)\n",
    "        grid_search = GridSearchCV(\n",
    "            model, param_grid, cv=inner_kf, scoring=\"neg_mean_squared_error\", verbose=3\n",
    "        )\n",
    "        grid_search.fit(X_train_outer, y_train_outer)\n",
    "\n",
    "        # Store the grid search results\n",
    "        param_combinations.extend(grid_search.cv_results_[\"params\"])\n",
    "        mse_values.extend(grid_search.cv_results_[\"mean_test_score\"])\n",
    "\n",
    "        # Get the best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # Predictions on the outer fold's test set\n",
    "        y_pred_outer = best_model.predict(X_test_outer)\n",
    "        outer_mses.append(mean_squared_error(y_test_outer, y_pred_outer))\n",
    "\n",
    "    return np.mean(outer_mses), param_combinations, mse_values\n",
    "\n",
    "\n",
    "# Parameter grid for RandomForest\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],  # Number of trees\n",
    "    \"max_depth\": [40, 50, 60],  # Depth of trees\n",
    "    \"min_samples_split\": [2, 5, 10],  # Minimum samples to split\n",
    "    \"n_jobs\": [-1],  # Use all processors\n",
    "    \"max_features\": [\"sqrt\"],  # Number of features to consider for the best split\n",
    "}\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_data(df, features, target)\n",
    "\n",
    "# Perform Nested Cross-Validation for RandomForest\n",
    "nested_mse, param_combinations, mse_values = nested_cv(\n",
    "    RandomForestRegressor(), param_grid, X_train, y_train\n",
    ")\n",
    "\n",
    "# Output the best parameters and the nested cross-validation MSE\n",
    "print(f\"Nested CV Mean MSE: {nested_mse}\")\n",
    "\n",
    "# Convert the results into a DataFrame for easier plotting\n",
    "results_df = pd.DataFrame(param_combinations)\n",
    "results_df[\"mse\"] = mse_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting\n",
    "# plt.figure(figsize=(12, 8))\n",
    "\n",
    "# # Plot MSE vs. n_estimators\n",
    "# plt.subplot(2, 2, 1)\n",
    "# for max_depth in param_grid[\"max_depth\"]:\n",
    "#     for min_samples_split in param_grid[\"min_samples_split\"]:\n",
    "#         subset = results_df[\n",
    "#             (results_df[\"max_depth\"] == max_depth)\n",
    "#             & (results_df[\"min_samples_split\"] == min_samples_split)\n",
    "#         ]\n",
    "#         plt.plot(\n",
    "#             subset[\"n_estimators\"],\n",
    "#             subset[\"mse\"],\n",
    "#             label=f\"max_depth={max_depth}, min_samples_split={min_samples_split}\",\n",
    "#         )\n",
    "\n",
    "# plt.title(\"MSE vs. Number of Estimators\")\n",
    "# plt.xlabel(\"Number of Estimators\")\n",
    "# plt.ylabel(\"Mean Squared Error\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Plot MSE vs. max_depth\n",
    "# plt.subplot(2, 2, 2)\n",
    "# for n_estimators in param_grid[\"n_estimators\"]:\n",
    "#     for min_samples_split in param_grid[\"min_samples_split\"]:\n",
    "#         subset = results_df[\n",
    "#             (results_df[\"n_estimators\"] == n_estimators)\n",
    "#             & (results_df[\"min_samples_split\"] == min_samples_split)\n",
    "#         ]\n",
    "#         plt.plot(\n",
    "#             subset[\"max_depth\"],\n",
    "#             subset[\"mse\"],\n",
    "#             label=f\"n_estimators={n_estimators}, min_samples_split={min_samples_split}\",\n",
    "#         )\n",
    "\n",
    "# plt.title(\"MSE vs. Max Depth\")\n",
    "# plt.xlabel(\"Max Depth\")\n",
    "# plt.ylabel(\"Mean Squared Error\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Plot MSE vs. min_samples_split\n",
    "# plt.subplot(2, 2, 3)\n",
    "# for n_estimators in param_grid[\"n_estimators\"]:\n",
    "#     for max_depth in param_grid[\"max_depth\"]:\n",
    "#         subset = results_df[\n",
    "#             (results_df[\"n_estimators\"] == n_estimators) & (results_df[\"max_depth\"] == max_depth)\n",
    "#         ]\n",
    "#         plt.plot(\n",
    "#             subset[\"min_samples_split\"],\n",
    "#             subset[\"mse\"],\n",
    "#             label=f\"n_estimators={n_estimators}, max_depth={max_depth}\",\n",
    "#         )\n",
    "\n",
    "# plt.title(\"MSE vs. Min Samples Split\")\n",
    "# plt.xlabel(\"Min Samples Split\")\n",
    "# plt.ylabel(\"Mean Squared Error\")\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting\n",
    "# plt.figure(figsize=(12, 8))\n",
    "\n",
    "# results_grouped = results_df.groupby([\"n_estimators\", \"max_depth\", \"min_samples_split\"])[\"mse\"].agg(\"mean\").reset_index()\n",
    "\n",
    "# # Plot MSE vs. n_estimators\n",
    "# plt.subplot(3, 1, 1)\n",
    "# for max_depth in param_grid['max_depth']:\n",
    "#     for min_samples_split in param_grid['min_samples_split']:\n",
    "#         subset = results_grouped[\n",
    "#             (results_grouped['max_depth'] == max_depth) &\n",
    "#             (results_grouped['min_samples_split'] == min_samples_split)\n",
    "#         ]\n",
    "#         plt.scatter(subset['n_estimators'], subset['mse'], label=f\"max_depth={max_depth}, min_samples_split={min_samples_split}\")\n",
    "\n",
    "# plt.title(\"MSE vs. Number of Estimators\")\n",
    "# plt.xlabel(\"Number of Estimators\")\n",
    "# plt.ylabel(\"Mean Squared Error\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Plot MSE vs. max_depth\n",
    "# plt.subplot(3, 1, 2)\n",
    "# for n_estimators in param_grid['n_estimators']:\n",
    "#     for min_samples_split in param_grid['min_samples_split']:\n",
    "#         subset = results_grouped[\n",
    "#             (results_grouped['n_estimators'] == n_estimators) &\n",
    "#             (results_grouped['min_samples_split'] == min_samples_split)\n",
    "#         ]\n",
    "#         plt.plot(subset['max_depth'], subset['mse'], label=f\"n_estimators={n_estimators}, min_samples_split={min_samples_split}\")\n",
    "\n",
    "# plt.title(\"MSE vs. Max Depth\")\n",
    "# plt.xlabel(\"Max Depth\")\n",
    "# plt.ylabel(\"Mean Squared Error\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Plot MSE vs. min_samples_split\n",
    "# plt.subplot(3, 1, 3)\n",
    "# for n_estimators in param_grid['n_estimators']:\n",
    "#     for max_depth in param_grid['max_depth']:\n",
    "#         subset = results_grouped[\n",
    "#             (results_grouped['n_estimators'] == n_estimators) &\n",
    "#             (results_grouped['max_depth'] == max_depth)\n",
    "#         ]\n",
    "#         plt.plot(subset['min_samples_split'], subset['mse'], label=f\"n_estimators={n_estimators}, max_depth={max_depth}\")\n",
    "\n",
    "# plt.title(\"MSE vs. Min Samples Split\")\n",
    "# plt.xlabel(\"Min Samples Split\")\n",
    "# plt.ylabel(\"Mean Squared Error\")\n",
    "# plt.tight_layout()\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
